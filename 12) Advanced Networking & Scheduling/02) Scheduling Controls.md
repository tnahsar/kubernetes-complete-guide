# ğŸ“˜ Chapter 29: Scheduling Controls

Now we are entering **smart cluster management** territory.

Until now, Kubernetes was automatically placing Pods on nodes for us.

But in real production, sometimes you need to control:

* â“ Which node runs which Pod
* â“ Which Pods should NOT run together
* â“ Which nodes are reserved for special workloads
* â“ How many Pods must stay available during maintenance

This chapter explains how to control scheduling like a pro.

---

# ğŸ§  First: How Scheduling Normally Works

Remember from Chapter 5:

The **Scheduler** (part of control plane) decides:

> "On which node should this Pod run?"

By default, it checks:

* CPU availability
* Memory availability
* Taints
* Constraints

Then places the Pod on the â€œbestâ€ node.

But sometimes default behavior is not enough.

Thatâ€™s where scheduling controls come in.

---

# ğŸ· 1ï¸âƒ£ Node Selectors (Basic Control)

## ğŸ“Œ What Is It?

A simple way to tell Kubernetes:

> "Run this Pod only on nodes with this label."

---

## ğŸ”¹ Step 1: Label a Node

```bash
kubectl label nodes node1 disktype=ssd
```

Now node1 has:

```
disktype=ssd
```

---

## ğŸ”¹ Step 2: Use nodeSelector in Pod

```yaml
spec:
  nodeSelector:
    disktype: ssd
```

Now the Pod will:

* Only run on nodes labeled `disktype=ssd`
* Fail if none available

---

## ğŸ§  When To Use

* GPU nodes
* SSD storage nodes
* Dedicated production nodes
* High-memory nodes

---

## ğŸš¨ Limitation

NodeSelector is:

* Simple
* Hard requirement
* Not flexible

For advanced logic â†’ use Affinity (coming soon ğŸ‘‡)

---

# ğŸš« 2ï¸âƒ£ Taints & Tolerations (Node Protection System)

This is opposite of nodeSelector.

Instead of:

> Pod chooses node

Here:

> Node rejects Pods

---

## ğŸ§  Mental Model

Taint = â€œNo one allowed here unless special permission.â€

---

## ğŸ”¹ Add Taint to Node

```bash
kubectl taint nodes node1 key=value:NoSchedule
```

Now:

* No Pods will schedule on node1
* Unless they tolerate it

---

## ğŸ”¹ Add Toleration to Pod

```yaml
spec:
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "value"
    effect: "NoSchedule"
```

Now this Pod:

* Is allowed on that node

---

## ğŸ§  Effects of Taints

| Effect           | Meaning                 |
| ---------------- | ----------------------- |
| NoSchedule       | Donâ€™t schedule new Pods |
| PreferNoSchedule | Avoid if possible       |
| NoExecute        | Remove running Pods     |

---

## ğŸ­ Real Use Cases

* Dedicated nodes for database
* GPU nodes
* System nodes
* Maintenance mode

Very powerful in production.

---

# ğŸ¯ 3ï¸âƒ£ Affinity & Anti-Affinity (Advanced Placement Rules)

This is advanced version of nodeSelector.

Two types:

### ğŸŸ¢ Node Affinity

Control which nodes Pods can run on.

### ğŸ”µ Pod Affinity

Control which Pods run together.

### ğŸ”´ Pod Anti-Affinity

Prevent Pods from running together.

---

## ğŸŸ¢ Node Affinity

More expressive than nodeSelector.

You can say:

* Must match
* Prefer to match
* Complex conditions

Example:

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd
```

Now:

* Only schedule on SSD nodes
* More flexible logic possible

---

## ğŸ”µ Pod Affinity

Example:

> Place frontend Pod near backend Pod (same node or zone)

Useful for:

* Low latency communication

---

## ğŸ”´ Pod Anti-Affinity (Very Important)

Example:

> Never place 2 replicas of same app on same node.

Why?

If node crashes â†’ you lose all replicas ğŸ˜±

Anti-affinity spreads Pods across nodes.

Production essential.

---

# ğŸ›‘ 4ï¸âƒ£ Pod Disruption Budgets (PDB)

Now this is about **availability during maintenance**.

Imagine:

You have 5 replicas.

Cluster admin upgrades nodes.

Some Pods must be evicted.

If all Pods stop â†’ downtime ğŸ˜¬

PDB prevents that.

---

## ğŸ§  What Is PDB?

It defines:

> Minimum number of Pods that must stay available.

---

## Example:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
spec:
  minAvailable: 2
```

Meaning:

* At least 2 Pods must stay running
* Kubernetes will not allow more disruptions

---

## ğŸ­ When Used?

* During node upgrades
* During cluster scaling
* During voluntary disruptions

Important:

PDB does NOT protect from crashes.
It protects from planned disruptions.

---

# ğŸ§  How All Scheduling Controls Work Together

Order of evaluation:

1. Node resources check
2. Taints
3. NodeSelector
4. Affinity rules
5. PDB during eviction

Everything combined controls Pod placement.

---

# ğŸ† Real Production Example

Imagine:

* 3 nodes
* 3 replicas
* Anti-affinity enabled
* PDB minAvailable=2

Result:

* Pods spread across nodes
* During upgrade, only 1 can go down
* App remains available

Thatâ€™s production-grade reliability.

---

# ğŸš¨ Common Beginner Mistakes

âŒ Overusing hard rules
âŒ Forgetting anti-affinity
âŒ Using only nodeSelector in production
âŒ No PDB defined
âŒ Blocking scheduling accidentally with taints

---

# ğŸ“Œ Chapter 29 Summary

Scheduling Controls allow you to:

* Control where Pods run
* Protect special nodes
* Spread Pods safely
* Maintain high availability
* Handle maintenance safely

Tools covered:

* NodeSelector (basic filtering)
* Taints & Tolerations (node protection)
* Affinity & Anti-Affinity (advanced logic)
* Pod Disruption Budgets (availability control)

---

Next we move to:

# ğŸ“˜ Chapter 30: Logging & Monitoring Concepts

(Where we finally learn how to observe production clusters ğŸ‘€ğŸ“Š)

Ready to continue?
