# ğŸ“˜ Chapter 43: Real-World Kubernetes Scenarios

*(Production Situations, Scaling, Failures, Recovery)*

Now this is the final and most practical chapter.

Interviews at mid/senior level are no longer about definitions.

They ask:

> â€œTell me what you would do in a real production scenario.â€

This chapter prepares you for that.

---

# ğŸš€ Scenario 1: Traffic Spike During Sale / Event

### â“ Situation:

Your application suddenly receives 10x traffic.

Pods are running at 90â€“100% CPU.

---

## ğŸ§  What Should You Do?

### Step 1: Check Metrics

* CPU usage
* Memory usage
* HPA status

```bash
kubectl get hpa
```

---

### Step 2: If HPA Exists

If HPA configured properly:

* It automatically increases replicas.

If maxReplicas reached:

* Increase maxReplicas
* Or scale nodes (Cluster Autoscaler)

---

### Step 3: If No HPA

Manually scale:

```bash
kubectl scale deployment myapp --replicas=10
```

Then implement HPA permanently.

---

## ğŸ¯ Interview Answer

> â€œIf traffic spikes, I check HPA and metrics first. If autoscaling is configured, I verify scaling behavior. If not, I manually scale and later implement HPA to prevent recurrence.â€

That shows maturity.

---

# ğŸ’¥ Scenario 2: Pod Keeps Crashing (CrashLoopBackOff)

### â“ Situation:

Pod status shows:

```
CrashLoopBackOff
```

---

## ğŸ§  Troubleshooting Steps

1. Check logs:

```bash
kubectl logs pod-name
```

2. Describe pod:

```bash
kubectl describe pod pod-name
```

3. Check:

* Image pull errors
* Environment variables
* Secrets mounted?
* Resource limits exceeded?

---

## Common Causes

* Wrong DB connection string
* Secret missing
* OOMKilled (memory exceeded)
* Wrong startup command

---

## ğŸ¯ Interview Answer

> â€œI check logs first, then describe the pod for events. Most CrashLoop issues are due to misconfiguration, missing secrets, or memory limits.â€

Structured + practical = strong answer.

---

# ğŸ—„ Scenario 3: Database Pod Crashes

### â“ Situation:

Production database pod went down.

---

## ğŸ§  What Should Be in Place?

* StatefulSet
* PersistentVolumeClaim
* Readiness probe
* Backups

---

## Immediate Action

1. Check pod status
2. Verify PVC attached
3. Check disk space
4. Restore from backup if required

---

## ğŸ¯ Interview-Level Response

> â€œDatabase workloads should run as StatefulSet with persistent storage. If it crashes, I verify storage attachment and restore from backup if necessary.â€

---

# ğŸ”¥ Scenario 4: Node Failure

### â“ Situation:

One worker node goes down.

---

## What Happens Automatically?

* Control plane detects node NotReady
* Pods rescheduled to other nodes
* Services redirect traffic

---

## What Should You Verify?

* Enough nodes available
* Resource capacity sufficient
* Cluster Autoscaler active

---

## ğŸ¯ Strong Answer

> â€œKubernetes automatically reschedules pods to healthy nodes. I verify resource capacity and ensure autoscaling is enabled.â€

---

# ğŸš¨ Scenario 5: Deployment Caused Outage

### â“ Situation:

New version deployed â†’ application down.

---

## ğŸ§  Steps

1. Check rollout status:

```bash
kubectl rollout status deployment myapp
```

2. If failing:

```bash
kubectl rollout undo deployment myapp
```

3. Investigate logs

---

## Why This Works?

Deployment stores previous ReplicaSets.

Rollback restores previous version instantly.

---

## ğŸ¯ Interview Answer

> â€œIf a deployment fails, I immediately rollback using kubectl rollout undo and then analyze logs to identify root cause.â€

This shows calm under pressure.

---

# ğŸ§¯ Scenario 6: Memory OOMKilled

### â“ Situation:

Pods keep restarting due to OOMKilled.

---

## ğŸ§  Root Cause

Memory limit too low.

---

## Solution

* Increase memory limit
* Optimize application memory usage
* Monitor memory consumption

---

## Interview-Level Response

> â€œOOMKilled indicates memory limit breach. I check resource limits and tune memory allocation accordingly.â€

---

# ğŸ” Scenario 7: Developer Needs Access to Cluster

### â“ Situation:

New developer joins team.

---

## Correct Approach

* Create Role (namespace-level)
* Bind Role to user
* Avoid cluster-admin

---

## Interview Answer

> â€œI follow least privilege principle and grant namespace-level access using RBAC instead of cluster-admin.â€

Security mindset matters.

---

# ğŸ“¦ Scenario 8: Multi-Environment Deployment

### â“ Situation:

You need dev, staging, prod environments.

---

## Best Practice

* Separate namespaces
* Separate values files
* Separate CI pipeline stages

---

## Example

```
values-dev.yaml
values-staging.yaml
values-prod.yaml
```

---

# ğŸ“Š Scenario 9: Monitoring Not Set Up

### â“ Situation:

Application slow, no monitoring.

---

## Immediate Action

* Check pod metrics
* Check node metrics
* Add Prometheus + Grafana
* Enable alerts

---

## Interview Response

> â€œProduction without monitoring is blind. I ensure centralized logging and metrics collection are implemented.â€

---

# ğŸ§  Senior-Level Scenario Thinking

When answering scenario questions:

Always structure answer like:

1. Immediate action
2. Investigation steps
3. Long-term prevention

Example:

> â€œFirst I stabilize the system, then identify root cause, then implement preventive measures.â€

Interviewers love this structure.

---

# ğŸ You Have Completed

You now know:

* Kubernetes from scratch
* Architecture
* Objects
* Networking
* Storage
* Scaling
* Security
* Helm
* CI/CD
* Troubleshooting
* Real production scenarios

This is not beginner knowledge anymore.

This is:

> ğŸ”¥ Real-world DevOps Kubernetes mastery.

---

# ğŸ¯ Final Question For You

Would you like to:

* ğŸ“Œ Create a 1-week revision plan?
* ğŸ“Œ Create mock interview session?
* ğŸ“Œ Create senior-level deep-dive scenarios?
* ğŸ“Œ Or move to Kubernetes certification prep (CKA style)?

Youâ€™ve built something serious here.
